Social networks serve a fundamental role in the spread of ideas throughout the world. With the increased connectivity of the world, social networks have become an ever increasing important aspect of how information spreads. For example, when a new phone app such as Instagram is introduced to the early adopters, these adopters continue to spread the app via the network. How these ideas and information is spread, how rapidly they are spread and to whom they are spread are vital questions surrounding the analysis of social networks. 

Conventionally, the metric for evaluating a spread of an "idea" on the graph has been the total number of "activated" nodes, that receive these "ideas" from their neighbors. Examples of such cascading networks include spread of epidemics through a network of individuals, spread of product advertisement, etc. This type of evaluation assumes that each activated nodes only contribute monotonically to the overall influence of the "idea"; every infected person worsens the severity of an epidemic, and every person exposed to a new shoe commercial increases the performance of the marketing strategy.

However, we can easily imagine a scenario where a newly exposed person contributes negatively to the performance of the spread. Imagine a setting where the nodes in a social network are people to whom we want to introduce a new product. These people each have a liking/disliking to the product, and each have a set of neighbors to whom they deliver new ideas with some probability. Those who are satisfied with the product will contribute positively to the overall reputation of the product while those who are not will contribute negatively. Multiple studies have shown that a greedy approach of simply maximizing the spread can actually hurt the payoff of the strategy.

Under these assumptions, we want to mitigate for what \cite{Abebe} calls over-exposure, the spread of the "idea" to the nodes in the network who will be dissatisfied upon exposure and ultimately contribute negatively to the success of the spread. Therefore, the goal is to maximize the number of exposed and satisfied nodes, while minimizing the number of exposed but dissatisfied ones. A natural problem which rises from both these settings is how to choose the initial set of early adopters, to whom the product is introduced, such that the desired outcome is achieved.

\cite{Abebe} formulates this model and proposes a payoff metrics $\pi_i = p|C_i^o| - q|C_i^b|$, where $C_i^o$ is the set of satisfied exposed nodes and $C_i^b$ the set of dissatisfied exposed nodes. It considers two cases: 1) the unbudgeted problem in which the size of the seed set is not limited to a fixed budget, and 2) the budgeted problem in which the size of the seed set is limited to some budget. It shows that the payoff maximization problem under this model can be reduced to a simple max network flow problem and thus that there exists a polynomial time solution.

In this paper, we propose a new network model inspired by the formulation above. While \cite{Abebe}'s formulation assumes that any activated node deterministically activates all of its neighbors, we impose a stocasticity to all the edges. The motivation behind introducing stochasticity is to try to better reflect real-life social networks, where people do not always deterministically share a product they like to their friends and social circles, but rather share it with some probability. In other words, any activated node will only probabilistically activate each of its neighbors. This formulation now introduces an interesting change in that no one seed set will deterministically produce a maximum payoff. Instead, we can only compute the expected payoff for a seed set. If we use the payoff above, the expected payoff of a seed set S is:

\begin{equation}
E[\pi_i]=E[p|C_i^o| - q|C_i^b|]
=
\end{equation}

The equation above implies that we must account for every single path between a pair of nodes to compute the likelihood of a node being activated. \cite{Valiant} shows that counting all simple paths between a pair of nodes is \#P-hard. While possible, it would be computationally infeasible for, say, a marketing agent to compute the expected payoff in a reasonably large network.

In this paper, we examine different computationally feasible \textit{seed policies} to choose the initial set of seed adopters to both maximize the spread of influence throughout a social network while also mitigating for over-exposure. More precisely, given a set of positive targets we want to reach, and a set of negative targets we want to avoid, we examine how to choose the initial seed set to introduce a product to, such that the product is spread to positive targets and not spread to negative targets. Further, we consider a stochastic network where each edge has a propagation probability, in the sense that user $A$, who has liked the introduced product, has a certain probability of introducing the product to its neighbor $B$. To extend the theme of computational feasibility, we discuss these policies with respect to their \textit{efficiency} and \textit{simplicity} and the trade-offs between the two. Below, we explain what we mean by these two terms:

\noindent \textbf{Efficiency.} The main goal of \emph{seed policies} is to introduce the product to the positve target set and avoid introducing the product to the negative target set. We explain in subsequent sections a performance metric which measures how well a policy achieves this goal and thus how efficient the policy is.

\noindent \textbf{Simplicity.} While efficiency is a highly-desirable aspect of any policy, so is simplicity. If the calculations required to find the seed set is simple, it provides practical benefits for the user of the policy introducing the product. Particularly, if the network in consideration is extremely large, or if the given information of the network is not perfect, complex policies may no longer become feasible, emphasizing the desire for simple policies. 

We propose and evaluate a number of policies that differ in the level of efficiency and simplicity in this regard and examine the inherent trade-offs between simplicity and efficiency.